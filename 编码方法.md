# 脉冲编码

## 一、rate-based
ANN传统网络，输出的数值可看成rate，在转换中我们实质需要的权重，训练得到权重值得范围取决了转换成SNN的损失情况。

## 二、spike-based
SNN是以脉冲传输信息的，以上面的 rate code进行编码绘图可得出上面ANN激活后对应的图，实质上不论事用rate code 或 spike code 其本质都是一样，在实际中在SNN用何种编码是由模拟器底层实现，不论用何种编码都能将ANN权重应用到SNN

## 三、event-driven或event-based

SNN运行平台由来自神经形态传感器或将图片转换成脉冲的输入，创建稀疏，无帧和精确定时的事件流，模型LIF和IF，我们把每个脉冲的产生看成一个事件，所以说SNN是以事件驱动的，事件驱动的神经元系统将其计算工作集中在网络的当前活动部分，由于输入是一连串的脉冲，所以可以看成整个网络的神经元都在进行工作，但他们只关注其自己当前所接受的东西，所以相对于ANN他是低延迟

frame-based
对于ANN传统网络而言，与上述event-driven对比，ANN是在二进制计算机上运行，每次传输都是一个实际的数值，对于训练的一张图片而言，其每个像素都是一个实际值，然后逐层的通过网络，当通过第一层时，后面层的神经元不工作，当通过到第三层时，第一二层和后面的层都不工作，所以延迟高。

ANN转SNN中，我们实际需要的只有ANN中训练的权重，权重又受到ANN中的激活函数，dropout，正则化，池化，权重归一化（如batch Normalization，还有很多类型），输入数据的归一化范围之类的影响。这些技术都会影响到权重值得取值。一般来说，ANN转SNN后，准确度肯定会下降，我们要做的就是尽量将损失减低。而损失的多少直接由权重决定。

ANN转SNN使用的网络一般是两种，一种是全连接网络（代表是DBN），一种是卷积网络（代表CNN）。
当使用CNN一般要进行网络的”修剪“
1、max pool一般用average pool代替，
2、确保网络中不出现负值（全连接网络中也要）
3、将偏置bias设置为0（全连接网络中也要）
实质上在负值在SNN中也是能表示的，即bias不设置为0也是可以的，但这样需要引入大量的抑制性神经元，一般不这么做

ANN转SNN有两种类型，是根据SNN采用的模型来进行区别，一种IF模型，另一种LIF模型。LIF相比IF更具有生物真实性。
1、采用IF的SNN，已经可以将损失减到好少（无论应用在全连接网络还是卷积网络），其准确度已经和ANN训练的结果非常接近。这种方法，一般在ANN直接使用relu激活函数进行训练，当然在然后采用一些自定义的权重归一化技术，L1、L2正则化、dropout，对输入数据进行处理等技术，使权重尽量适合于SNN。这种方法已经比较成熟，损失已经非常少，所以人们开始将研究方向转向更具有生物真实性的LIF。

2、采用LIF的SNN，采用这种模型，我们在ANN网络中不在采用relu激活函数，一般是利用LIF模型的响应方程，自己推导出一个与LIF响应方程尽量相似的激活函数。这是这种方法的最难的地方。当前研究一般是将其应用到全连接的网络中。

SNN讨论内容,一是能耗，另一是延迟。
1、能耗，研究整个网络在一次分类中所产生的总脉冲数，然后利用已知的每个脉冲的能耗，之类的定值计算出总能耗。能耗也可以看成计算量，具有不同网络结构或相同网络结构参数不同（后者主要是针对一些已经训练好的SNN，对数据库进行不同的速率进行脉冲转换）计算各自的产生的脉冲数（计算量）

2、延迟，延迟与SNN模型设置的阈值有很大关系，一般延迟低分类准确性就较低，延迟时间加长分类准确度会上升，这通过我们设置模型的阈值，如果想要高准确度，可以设置高的阈值，这时需要接受较多的脉冲才能激发，因此延迟时间加长，如果想要延迟，就设置低的阈值，这是只要接受几个脉冲就可以激发，延迟低了，但准确性低了。



